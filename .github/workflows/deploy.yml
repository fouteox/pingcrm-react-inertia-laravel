name: Build and Deploy

on:
  pull_request:
    types: [closed]
    branches: [ main ]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build:
    runs-on: ${{ vars.SYSTEM_ARCH == 'aarch64' && 'ubuntu-24.04-arm' || 'ubuntu-latest' }}
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Create .env.production file
        run: echo "${{ secrets.ENV_FILE_BASE64 }}" | base64 -d > .env.production

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for App image
        id: meta-app
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=sha,format=short,suffix=-app
            type=ref,event=branch,suffix=-app
            latest

      - name: Build and push App image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          target: app
          tags: ${{ steps.meta-app.outputs.tags }}
          labels: ${{ steps.meta-app.outputs.labels }}
          cache-from: |
            type=gha,scope=app
            type=gha,scope=shared
          cache-to: |
            type=gha,mode=max,scope=app
            type=gha,mode=max,scope=shared

      - name: Extract metadata for SSR image
        id: meta-ssr
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=sha,format=short,suffix=-ssr
            type=ref,event=branch,suffix=-ssr
            ssr

      - name: Build and push SSR image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          target: ssr
          tags: ${{ steps.meta-ssr.outputs.tags }}
          labels: ${{ steps.meta-ssr.outputs.labels }}
          cache-from: |
            type=gha,scope=ssr
            type=gha,scope=shared
          cache-to: |
            type=gha,mode=max,scope=ssr
            type=gha,mode=max,scope=shared

  deploy:
    needs: build
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    env:
      USE_CF_TUNNEL: ${{ secrets.USE_CLOUDFLARE_TUNNEL }}

    steps:
      - name: Install cloudflared
        if: ${{ env.USE_CF_TUNNEL == 'true' }}
        run: |
          curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
          sudo dpkg -i cloudflared.deb

      - name: Setup SSH configuration (Cloudflare tunnel)
        if: ${{ env.USE_CF_TUNNEL == 'true' }}
        uses: shimataro/ssh-key-action@v2
        with:
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          name: id_ed25519
          known_hosts: unnecessary
          config: |
            Host ${{ secrets.SSH_HOST }}
              ProxyCommand /usr/local/bin/cloudflared access ssh --hostname %h
              User ${{ secrets.SSH_USER }}
              IdentityFile ~/.ssh/id_ed25519
              StrictHostKeyChecking accept-new

      - name: Setup SSH configuration (Direct)
        if: ${{ env.USE_CF_TUNNEL != 'true' }}
        uses: shimataro/ssh-key-action@v2
        with:
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          name: id_ed25519
          known_hosts: unnecessary
          config: |
            Host ${{ secrets.SSH_HOST }}
              User ${{ secrets.SSH_USER }}
              Port ${{ secrets.SSH_PORT }}
              IdentityFile ~/.ssh/id_ed25519
              StrictHostKeyChecking accept-new

      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Create .env.production file
        run: echo "${{ secrets.ENV_FILE_BASE64 }}" | base64 -d > .env.production

      - name: Create project directory and copy files
        env:
          REPO_SHORT_NAME: ${{ github.event.repository.name }}
        run: |
          ssh ${{ secrets.SSH_HOST }} "mkdir -p ~/$REPO_SHORT_NAME"
          scp compose.prod.yaml .env.production ${{ secrets.SSH_HOST }}:~/$REPO_SHORT_NAME/

      - name: Deploy with Docker Compose
        env:
          REPO_SHORT_NAME: ${{ github.event.repository.name }}
          FULL_IMAGE_NAME: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        run: |
          ssh ${{ secrets.SSH_HOST }} << DEPLOY_SCRIPT
            set -ex
            echo "=== [1/10] Starting deployment script ==="
            cd ~/${REPO_SHORT_NAME}
            source .env.production
            PROJECT_NAME="\${COMPOSE_PROJECT_NAME:-${REPO_SHORT_NAME}}"
            echo "PROJECT_NAME=\$PROJECT_NAME"

            echo "=== [2/10] Detecting services ==="
            DB_SERVICE="\${DB_CONNECTION}"
            CACHE_SERVICE="\${REDIS_HOST:-}"
            echo "DB_SERVICE=\$DB_SERVICE, CACHE_SERVICE=\$CACHE_SERVICE"

            # Map to volume names
            case "\$DB_SERVICE" in
              pgsql) DB_VOLUME="stack-pgsql" ;;
              mysql) DB_VOLUME="stack-mysql" ;;
              mariadb) DB_VOLUME="stack-mariadb" ;;
              *) DB_VOLUME="" ;;
            esac
            case "\$CACHE_SERVICE" in
              redis) CACHE_VOLUME="stack-redis" ;;
              valkey) CACHE_VOLUME="stack-valkey" ;;
              *) CACHE_VOLUME="" ;;
            esac
            echo "DB_VOLUME=\$DB_VOLUME, CACHE_VOLUME=\$CACHE_VOLUME"

            echo "=== [3/10] Checking first deployment ==="
            FIRST_DEPLOY="no"
            if [ -n "\$DB_VOLUME" ] && ! docker volume inspect "\${PROJECT_NAME}_\${DB_VOLUME}" > /dev/null 2>&1; then
              FIRST_DEPLOY="yes"
              echo "First deployment detected"
            fi
            echo "FIRST_DEPLOY=\$FIRST_DEPLOY"

            echo "=== [4/10] Login and pull images ==="
            echo "${{ secrets.GITHUB_TOKEN }}" | docker login ${{ env.REGISTRY }} -u ${{ github.actor }} --password-stdin
            export IMAGE_NAME=${FULL_IMAGE_NAME}
            docker compose -f compose.prod.yaml --env-file .env.production pull

            echo "=== [5/10] Start data services ==="
            SERVICES=""
            [ -n "\$DB_SERVICE" ] && [ "\$DB_SERVICE" != "sqlite" ] && SERVICES="\$DB_SERVICE"
            [ -n "\$CACHE_SERVICE" ] && SERVICES="\$SERVICES \$CACHE_SERVICE"
            echo "Starting services: \$SERVICES"
            [ -n "\$SERVICES" ] && docker compose -f compose.prod.yaml --env-file .env.production up -d --wait \$SERVICES

            echo "=== [6/10] Check backup restoration ==="
            if [ "\$FIRST_DEPLOY" = "yes" ] && [ -n "\${BACKUP_S3_BUCKET:-}" ]; then
              echo "Checking for existing backups..."

              BACKUP_EXISTS="no"
              if docker run --rm \
                -e AWS_ACCESS_KEY_ID="\${BACKUP_AWS_ACCESS_KEY_ID}" \
                -e AWS_SECRET_ACCESS_KEY="\${BACKUP_AWS_SECRET_ACCESS_KEY}" \
                -e AWS_ENDPOINT_URL="https://\${BACKUP_AWS_ENDPOINT}" \
                amazon/aws-cli s3 ls "s3://\${BACKUP_S3_BUCKET}/\${BACKUP_S3_PATH}/" 2>/dev/null | grep -q '\.tar\.gz'; then
                BACKUP_EXISTS="yes"
              fi
              echo "BACKUP_EXISTS=\$BACKUP_EXISTS"

              if [ "\$BACKUP_EXISTS" = "yes" ]; then
                echo "=== [7/10] Download backup ==="
                LATEST_BACKUP=\$(docker run --rm \
                  -e AWS_ACCESS_KEY_ID="\${BACKUP_AWS_ACCESS_KEY_ID}" \
                  -e AWS_SECRET_ACCESS_KEY="\${BACKUP_AWS_SECRET_ACCESS_KEY}" \
                  -e AWS_ENDPOINT_URL="https://\${BACKUP_AWS_ENDPOINT}" \
                  amazon/aws-cli s3 ls "s3://\${BACKUP_S3_BUCKET}/\${BACKUP_S3_PATH}/" \
                  | grep '\.tar\.gz' | sort | tail -n 1 | awk '{print \$4}')
                echo "LATEST_BACKUP=\$LATEST_BACKUP"

                if [ -n "\$LATEST_BACKUP" ]; then
                  echo "Downloading \$LATEST_BACKUP..."
                  docker run --rm \
                    -v "\${PROJECT_NAME}_backup_dumps:/restore" \
                    -e AWS_ACCESS_KEY_ID="\${BACKUP_AWS_ACCESS_KEY_ID}" \
                    -e AWS_SECRET_ACCESS_KEY="\${BACKUP_AWS_SECRET_ACCESS_KEY}" \
                    -e AWS_ENDPOINT_URL="https://\${BACKUP_AWS_ENDPOINT}" \
                    amazon/aws-cli s3 cp "s3://\${BACKUP_S3_BUCKET}/\${BACKUP_S3_PATH}/\${LATEST_BACKUP}" /restore/backup.tar.gz

                  echo "Extracting backup..."
                  docker run --rm \
                    -v "\${PROJECT_NAME}_backup_dumps:/restore" \
                    alpine sh -c "tar -xzf /restore/backup.tar.gz -C /restore --strip-components=2 backup/dumps/ && rm /restore/backup.tar.gz"
                  echo "Backup extracted successfully"
                fi

                echo "=== [8/10] Restore database ==="
                case "\$DB_SERVICE" in
                  pgsql)
                    if docker compose -f compose.prod.yaml --env-file .env.production exec -T pgsql test -f /dumps/pgsql.dump 2>/dev/null; then
                      echo "Restoring PostgreSQL..."
                      docker compose -f compose.prod.yaml --env-file .env.production exec -T pgsql \
                        pg_restore -U "\${DB_USERNAME}" -d "\${DB_DATABASE}" -c --if-exists /dumps/pgsql.dump || true
                      echo "PostgreSQL restore done"
                    fi
                    ;;
                  mysql)
                    if docker compose -f compose.prod.yaml --env-file .env.production exec -T mysql test -f /dumps/mysql.sql 2>/dev/null; then
                      echo "Restoring MySQL..."
                      docker compose -f compose.prod.yaml --env-file .env.production exec -T mysql \
                        sh -c 'mysql -u"\$MYSQL_USER" -p"\$MYSQL_PASSWORD" "\$MYSQL_DATABASE" < /dumps/mysql.sql' || true
                      echo "MySQL restore done"
                    fi
                    ;;
                  mariadb)
                    if docker compose -f compose.prod.yaml --env-file .env.production exec -T mariadb test -f /dumps/mariadb.sql 2>/dev/null; then
                      echo "Restoring MariaDB..."
                      docker compose -f compose.prod.yaml --env-file .env.production exec -T mariadb \
                        sh -c 'mariadb -u"\$MARIADB_USER" -p"\$MARIADB_PASSWORD" "\$MARIADB_DATABASE" < /dumps/mariadb.sql' || true
                      echo "MariaDB restore done"
                    fi
                    ;;
                esac

                echo "=== [9/10] Restore cache ==="
                if [ -n "\$CACHE_SERVICE" ]; then
                  if docker compose -f compose.prod.yaml --env-file .env.production exec -T \$CACHE_SERVICE test -f /dumps/\${CACHE_SERVICE}.rdb 2>/dev/null; then
                    echo "Restoring \$CACHE_SERVICE..."
                    docker compose -f compose.prod.yaml --env-file .env.production exec -T \$CACHE_SERVICE sh -c "cp /dumps/\${CACHE_SERVICE}.rdb /data/dump.rdb" || true
                    echo "Copied rdb file, now restarting..."
                    docker compose -f compose.prod.yaml --env-file .env.production restart \$CACHE_SERVICE || true
                    echo "Cache service restarted"
                  else
                    echo "No cache dump found"
                  fi
                else
                  echo "No cache service configured"
                fi
                echo "Backup restoration complete"
              else
                echo "No backup found in S3/R2 - starting with fresh database"
              fi
            else
              echo "Skipping backup restoration (FIRST_DEPLOY=\$FIRST_DEPLOY, BACKUP_S3_BUCKET=\${BACKUP_S3_BUCKET:-not set})"
            fi

            echo "=== [10/10] Start all services ==="
            docker compose -f compose.prod.yaml --env-file .env.production up -d --wait
            echo "=== Deployment complete! ==="
          DEPLOY_SCRIPT
